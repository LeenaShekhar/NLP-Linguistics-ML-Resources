# NLP-ML-Resources

This repository contains links to resources (papers, ebooks, blog posts, and courses) that I have found useful over the years to learn about NLP and ML. 
It is almost impossible to keep updating the list of papers as there are many out there already and tens on them added daily to [ArXiv](https://arxiv.org/list/cs.CL/recent). Though, I will mention some papers that I have found extremely interesting. This section will be biased!

Note: This is a WIP and will be updated on a regular basis but with an _unknown_ periodicty. 

## BOOKS + STRUCTURED COURSES

1. Classic NLP Textbook: [Speech and Language Processing by Jurafsky & Martin](https://web.stanford.edu/~jurafsky/slp3/)
2. NLP textbook with ML: [Natural Language Processing by Eisenstein](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)
3. NN and DL for NLP: [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
4. Graduate level NLP course with emphasis on DL: [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/syllabus.html)
5. Book on theoretical ML: [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf)
6. Graduate level course to learn about DL framework, TensorFlow: [CS 20: Tensorflow for Deep Learning Research](http://web.stanford.edu/class/cs20si/syllabus.html)
7. Fast.ai MOOC matrials for practical ideas: [Practical Deep Learning For Coders](http://course.fast.ai/)
8. CNN quick points: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)

## PAPERS [in no particular order]

1. [Scripts, plans, and knowledge. Roger C Schank and Robert P Abelson. 1975.]
2. [Analogs of Linguistic Structure in Deep Representations](https://arxiv.org/pdf/1707.08139.pdf)
3. [A Structured Variational Autoencoder for Contextual Morphological Inflection](https://arxiv.org/pdf/1806.03746.pdf)
4. [A Framework for Representing Knowledge](http://courses.media.mit.edu/2004spring/mas966/Minsky%201974%20Framework%20for%20knowledge.pdf)
5. [Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate](https://arxiv.org/abs/1807.04783)
6. On Language and Connectionism: Analysis of a Parallel Distributed Processing Model of Language Acquisition

## BLOG POSTS
1. Introduction to LSTMS: [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
2. Vanishing gradients in RNNs and how to stop them: [https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html)
3. Gradient descent optimization techniques: [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)
4. Introduction to VAEs: [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) and [Tutorial on Variational Autoencoders](https://arxiv.org/pdf/1606.05908.pdf)
5. Convex optimization: [Parameter-Free learning and optimization algorithms](https://parameterfree.wordpress.com/)

FUN READS
1. Information theory and cross entropy: [Visual Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/)
2. Information bottleneck: [Steps Towards Understanding Deep Learning: The Information Bottleneck Connection](https://weberna.github.io/jekyll/update/2017/11/08/Information-Bottleneck-Part1.html)
3. Handling uncertainty in deep learning by using Bayesian ML concepts [Uncertainty in Deep Learning](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)
